{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
    "from keras.models import Model,Sequential\n",
    "from keras.optimizers import Adam,SGD,RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_size = 48\n",
    "folder_path = \"images//\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28824 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size  = 64\n",
    "\n",
    "datagen_train  = ImageDataGenerator(1/255)\n",
    "datagen_val = ImageDataGenerator(1/255)\n",
    "\n",
    "\n",
    "train_set = datagen_train.flow_from_directory(folder_path+\"train\",\n",
    "                                              target_size = (picture_size,picture_size),\n",
    "                                              batch_size=batch_size,\n",
    "                                              color_mode=\"grayscale\",\n",
    "                                              class_mode='categorical',\n",
    "                                              shuffle=True)\n",
    "\n",
    "\n",
    "test_set = datagen_val.flow_from_directory(folder_path+\"validation\",\n",
    "                                              target_size = (picture_size,picture_size),\n",
    "                                              batch_size=batch_size,\n",
    "                                              color_mode=\"grayscale\",\n",
    "                                              class_mode='categorical',\n",
    "                                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angry': 0,\n",
       " 'disgust': 1,\n",
       " 'fear': 2,\n",
       " 'happy': 3,\n",
       " 'neutral': 4,\n",
       " 'sad': 5,\n",
       " 'surprise': 6}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 48, 48, 256)       2560      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 48, 48, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 48, 48, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 24, 24, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 24, 24, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 2,369,799\n",
      "Trainable params: 2,367,239\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam,SGD,RMSprop\n",
    "# Initialising the CNN\n",
    "model = Sequential()\n",
    "\n",
    "# 1 - Convolution\n",
    "model.add(Conv2D(256,(3,3), padding='same', input_shape=(picture_size, picture_size,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "# 2nd Convolution layer\n",
    "model.add(Conv2D(256,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# 3rd Convolution layer\n",
    "model.add(Conv2D(256,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "# 4th Convolution layer\n",
    "model.add(Conv2D(256,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# 5th Convolution layer\n",
    "model.add(Conv2D(256,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "\n",
    "# Flattening\n",
    "model.add(Flatten())\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "opt = Adam(lr=0.0001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from keras.optimizers import Adam,SGD,RMSprop\\n# Initialising the CNN\\nmodel = Sequential()\\n\\n# 1 - Convolution\\nmodel.add(Conv2D(128,(3,3), padding='same', input_shape=(picture_size, picture_size, 3)))\\nmodel.add(BatchNormalization())\\nmodel.add(Activation('relu'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\n\\n# Flattening\\nmodel.add(Flatten())\\nmodel.add(Dense(7, activation='softmax'))\\n\\nopt = Adam(lr=0.001)\\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\\nmodel.summary()\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from keras.optimizers import Adam,SGD,RMSprop\n",
    "# Initialising the CNN\n",
    "model = Sequential()\n",
    "\n",
    "# 1 - Convolution\n",
    "model.add(Conv2D(128,(3,3), padding='same', input_shape=(picture_size, picture_size, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening\n",
    "model.add(Flatten())\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "451/451 [==============================] - 846s 2s/step - loss: 2.0629 - accuracy: 0.2367 - val_loss: 1.5707 - val_accuracy: 0.3928\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/60\n",
      "451/451 [==============================] - 846s 2s/step - loss: 1.6594 - accuracy: 0.3522 - val_loss: 1.4832 - val_accuracy: 0.4276\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/60\n",
      "451/451 [==============================] - 846s 2s/step - loss: 1.5246 - accuracy: 0.4078 - val_loss: 1.3966 - val_accuracy: 0.4690\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 1.4178 - accuracy: 0.4526 - val_loss: 1.3232 - val_accuracy: 0.4947\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 1.3512 - accuracy: 0.4799 - val_loss: 1.2929 - val_accuracy: 0.5055\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/60\n",
      "451/451 [==============================] - 846s 2s/step - loss: 1.2884 - accuracy: 0.5053 - val_loss: 1.2552 - val_accuracy: 0.5232\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/60\n",
      "451/451 [==============================] - 848s 2s/step - loss: 1.2547 - accuracy: 0.5216 - val_loss: 1.2186 - val_accuracy: 0.5359\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 1.2037 - accuracy: 0.5469 - val_loss: 1.2208 - val_accuracy: 0.5487\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 1.1486 - accuracy: 0.5674 - val_loss: 1.3197 - val_accuracy: 0.5136\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/60\n",
      "451/451 [==============================] - 849s 2s/step - loss: 1.1214 - accuracy: 0.5787 - val_loss: 1.1399 - val_accuracy: 0.5716\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 1.0949 - accuracy: 0.5926 - val_loss: 1.1160 - val_accuracy: 0.5824\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 1.0540 - accuracy: 0.6067 - val_loss: 1.1055 - val_accuracy: 0.5868\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 1.0257 - accuracy: 0.6169 - val_loss: 1.1306 - val_accuracy: 0.5797\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 0.9954 - accuracy: 0.6257 - val_loss: 1.1081 - val_accuracy: 0.5837\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/60\n",
      "451/451 [==============================] - 848s 2s/step - loss: 0.9800 - accuracy: 0.6336 - val_loss: 1.1132 - val_accuracy: 0.5915\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/60\n",
      "451/451 [==============================] - 848s 2s/step - loss: 0.9456 - accuracy: 0.6532 - val_loss: 1.0954 - val_accuracy: 0.5980\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/60\n",
      "451/451 [==============================] - 849s 2s/step - loss: 0.9193 - accuracy: 0.6581 - val_loss: 1.0879 - val_accuracy: 0.6114\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/60\n",
      "451/451 [==============================] - 848s 2s/step - loss: 0.8896 - accuracy: 0.6694 - val_loss: 1.1190 - val_accuracy: 0.5980\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 0.8634 - accuracy: 0.6824 - val_loss: 1.1159 - val_accuracy: 0.5980\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 0.8397 - accuracy: 0.6866 - val_loss: 1.0832 - val_accuracy: 0.6094\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 21/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 0.8102 - accuracy: 0.6968 - val_loss: 1.0842 - val_accuracy: 0.6064\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 22/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 0.7750 - accuracy: 0.7169 - val_loss: 1.0572 - val_accuracy: 0.6209\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 23/60\n",
      "451/451 [==============================] - 848s 2s/step - loss: 0.7570 - accuracy: 0.7168 - val_loss: 1.0602 - val_accuracy: 0.6168\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 24/60\n",
      "451/451 [==============================] - 846s 2s/step - loss: 0.7288 - accuracy: 0.7312 - val_loss: 1.0699 - val_accuracy: 0.6274\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 25/60\n",
      "451/451 [==============================] - 846s 2s/step - loss: 0.7237 - accuracy: 0.7296 - val_loss: 1.1068 - val_accuracy: 0.6288\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 26/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 0.6800 - accuracy: 0.7493 - val_loss: 1.0758 - val_accuracy: 0.6293\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 27/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 0.6692 - accuracy: 0.7506 - val_loss: 1.0618 - val_accuracy: 0.6294\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 28/60\n",
      "451/451 [==============================] - 846s 2s/step - loss: 0.6456 - accuracy: 0.7613 - val_loss: 1.0741 - val_accuracy: 0.6190\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 29/60\n",
      "451/451 [==============================] - 846s 2s/step - loss: 0.6313 - accuracy: 0.7668 - val_loss: 1.0930 - val_accuracy: 0.6324\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 30/60\n",
      "451/451 [==============================] - 845s 2s/step - loss: 0.5999 - accuracy: 0.7812 - val_loss: 1.1210 - val_accuracy: 0.6223\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 31/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 0.5821 - accuracy: 0.7859 - val_loss: 1.0773 - val_accuracy: 0.6364\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 32/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 0.5514 - accuracy: 0.7998 - val_loss: 1.1224 - val_accuracy: 0.6334\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 33/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 0.5389 - accuracy: 0.8044 - val_loss: 1.1549 - val_accuracy: 0.6202\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 34/60\n",
      "451/451 [==============================] - 846s 2s/step - loss: 0.5267 - accuracy: 0.8094 - val_loss: 1.1946 - val_accuracy: 0.6236\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 35/60\n",
      "451/451 [==============================] - 846s 2s/step - loss: 0.5085 - accuracy: 0.8105 - val_loss: 1.1503 - val_accuracy: 0.6344\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 36/60\n",
      "451/451 [==============================] - 846s 2s/step - loss: 0.4870 - accuracy: 0.8208 - val_loss: 1.1711 - val_accuracy: 0.6335\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 37/60\n",
      "451/451 [==============================] - 846s 2s/step - loss: 0.4779 - accuracy: 0.8251 - val_loss: 1.2040 - val_accuracy: 0.6288\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 38/60\n",
      "451/451 [==============================] - 846s 2s/step - loss: 0.4471 - accuracy: 0.8398 - val_loss: 1.1603 - val_accuracy: 0.6308\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 39/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 0.4415 - accuracy: 0.8399 - val_loss: 1.1547 - val_accuracy: 0.6359\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 40/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 0.4285 - accuracy: 0.8444 - val_loss: 1.1489 - val_accuracy: 0.6351\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 41/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 0.4066 - accuracy: 0.8508 - val_loss: 1.2466 - val_accuracy: 0.6297\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 42/60\n",
      "451/451 [==============================] - 846s 2s/step - loss: 0.3955 - accuracy: 0.8566 - val_loss: 1.2258 - val_accuracy: 0.6308\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 43/60\n",
      "451/451 [==============================] - 854s 2s/step - loss: 0.3849 - accuracy: 0.8621 - val_loss: 1.2354 - val_accuracy: 0.6359\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 44/60\n",
      "451/451 [==============================] - 850s 2s/step - loss: 0.3643 - accuracy: 0.8691 - val_loss: 1.1965 - val_accuracy: 0.6371\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 45/60\n",
      "451/451 [==============================] - 843s 2s/step - loss: 0.3625 - accuracy: 0.8693 - val_loss: 1.2921 - val_accuracy: 0.6334\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 46/60\n",
      "451/451 [==============================] - 843s 2s/step - loss: 0.3386 - accuracy: 0.8797 - val_loss: 1.2530 - val_accuracy: 0.6286\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 47/60\n",
      "451/451 [==============================] - 843s 2s/step - loss: 0.3355 - accuracy: 0.8781 - val_loss: 1.2590 - val_accuracy: 0.6391\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 48/60\n",
      "451/451 [==============================] - 843s 2s/step - loss: 0.3161 - accuracy: 0.8843 - val_loss: 1.2660 - val_accuracy: 0.6384\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 49/60\n",
      "451/451 [==============================] - 844s 2s/step - loss: 0.3114 - accuracy: 0.8860 - val_loss: 1.3310 - val_accuracy: 0.6233\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 50/60\n",
      "451/451 [==============================] - 847s 2s/step - loss: 0.2967 - accuracy: 0.8942 - val_loss: 1.2664 - val_accuracy: 0.6416\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 51/60\n",
      "451/451 [==============================] - 901s 2s/step - loss: 0.2794 - accuracy: 0.9000 - val_loss: 1.3014 - val_accuracy: 0.6369\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 52/60\n",
      "451/451 [==============================] - 859s 2s/step - loss: 0.2861 - accuracy: 0.8968 - val_loss: 1.3444 - val_accuracy: 0.6368\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 53/60\n",
      "451/451 [==============================] - 867s 2s/step - loss: 0.2759 - accuracy: 0.8999 - val_loss: 1.3718 - val_accuracy: 0.6337\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 54/60\n",
      "451/451 [==============================] - 895s 2s/step - loss: 0.2606 - accuracy: 0.9079 - val_loss: 1.3352 - val_accuracy: 0.6382\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 55/60\n",
      "451/451 [==============================] - 861s 2s/step - loss: 0.2637 - accuracy: 0.9050 - val_loss: 1.2628 - val_accuracy: 0.6490\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 56/60\n",
      "451/451 [==============================] - 862s 2s/step - loss: 0.2480 - accuracy: 0.9136 - val_loss: 1.3001 - val_accuracy: 0.6416\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 57/60\n",
      "451/451 [==============================] - 859s 2s/step - loss: 0.2442 - accuracy: 0.9130 - val_loss: 1.3799 - val_accuracy: 0.6408\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 58/60\n",
      "451/451 [==============================] - 883s 2s/step - loss: 0.2327 - accuracy: 0.9183 - val_loss: 1.4455 - val_accuracy: 0.6354\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 59/60\n",
      "451/451 [==============================] - 866s 2s/step - loss: 0.2333 - accuracy: 0.9174 - val_loss: 1.3592 - val_accuracy: 0.6456\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 60/60\n",
      "451/451 [==============================] - 901s 2s/step - loss: 0.2196 - accuracy: 0.9215 - val_loss: 1.3165 - val_accuracy: 0.6464\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=2, min_lr=0.0001, mode='auto')\n",
    "\n",
    "callbacks_list = [checkpoint,reduce_lr]\n",
    "\n",
    "history = model.fit(train_set,\n",
    "                    batch_size = 64,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = test_set,\n",
    "                    validation_steps = test_set.n//test_set.batch_size,\n",
    "                    callbacks=callbacks_list\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model4.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = tf.keras.models.load_model('model4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 33s 294ms/step - loss: 1.3152 - accuracy: 0.6469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3151878118515015, 0.6469006538391113]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2887c3a9ca0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_set[0][0][0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-071a880abeff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#PROBAMOS MODELO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mload_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#PROBAMOS MODELO\n",
    "load_model.predict(test_set)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "foto_test = cv2.imread('triste2.jpg', flags = cv2.IMREAD_GRAYSCALE)\n",
    "foto_test = cv2.resize(foto_test, (48, 48)) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2880b43d820>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(foto_test, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foto_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "foto_test_ok1 = foto_test.reshape(1, 48, 48, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 48, 48, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foto_test_ok1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.342e-01, 3.000e-04, 4.690e-02, 2.373e-01, 1.407e-01, 5.190e-02,\n",
       "        8.870e-02]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(load_model.predict(foto_test_ok1),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
